{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a9c157",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da34c599",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.initializers import Initializer\n",
    "keras.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d7eb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#env = gym.make('CartPole-v1')\n",
    "env = gym.make(\n",
    "            id=\"CartPole-v1\",  # Choose one of the existing environments\n",
    "            render_mode=\"rgb_array\",  # The set of supported modes varies per environment. (And some third-party environments may not support rendering at all.)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa41920d",
   "metadata": {},
   "source": [
    "Action Space is a [Discrete](https://gymnasium.farama.org/api/spaces/fundamental/#gymnasium.spaces.Discrete) type\n",
    "A space consisting of finitely many elements.\n",
    "In CartPole-v1, it has two actions: left or right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65ca180",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(env.action_space))\n",
    "print(f\"Action space size: {env.action_space.n}\")\n",
    "print(f\"Action space start value: {env.action_space.start}\")\n",
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81095b1a",
   "metadata": {},
   "source": [
    "Observation space is a 4 dimension state: \n",
    "- Cart Position\n",
    "- Cart Velocity\n",
    "- Pole Angle\n",
    "- Pole Velocity at tip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af424be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(env.observation_space))\n",
    "# print(f\"Action space size: {env.observation_space.n}\")\n",
    "# print(f\"Action space start value: {env.observation_space.start}\")\n",
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9324d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9f9058",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation,_ = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679dfdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_state, reward, terminated, truncated, info = env.step(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df866624",
   "metadata": {},
   "source": [
    "### Create CartPole environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9fac87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from q_learning_lab.domain.models.cart_pole_v1_models import Params as Cart_Pole_Params\n",
    "from pathlib import Path\n",
    "params = Cart_Pole_Params(\n",
    "            total_episodes=2000,\n",
    "            n_max_steps=100,\n",
    "            learning_rate=0.7, # learning rate alpha\n",
    "            gamma=0.618, # discount rate\n",
    "            epsilon=0.1,\n",
    "            savefig_folder=Path(\"_static/img/tutorials/\"),\n",
    "            savemodel_folder=Path(\"_static/model/tutorials/\"),\n",
    "            start_epsilon=1.0,  # Starting exploration probability\n",
    "            min_epsilon=0.05,  # Minimum exploration probability\n",
    "            decay_rate=0.001,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef39076",
   "metadata": {},
   "outputs": [],
   "source": [
    "from q_learning_lab.port.environment import create_execute_environment\n",
    "from q_learning_lab.domain.models.cart_pole_v1_models import get_dnn_structure\n",
    "from q_learning_lab.domain.deep_q_learn import Reinforcement_DeepLearning\n",
    "import os\n",
    "# Create the environment\n",
    "env = create_execute_environment(arena=\"CartPole-v1\", params=params)\n",
    "dnn_structure = get_dnn_structure(\n",
    "            input_dim=env.observation_space_dim,\n",
    "            output_dim=env.action_space_dim,\n",
    "        )\n",
    "deepagent_dict = Reinforcement_DeepLearning.train(\n",
    "            env=env,\n",
    "            params=params,\n",
    "            dnn_structure=dnn_structure,\n",
    "            is_verbose=True,\n",
    "        )\n",
    "model_path = os.path.join(\n",
    "            params.savemodel_folder, \"interactive\", \"CartPole-v1-main\"\n",
    "        )\n",
    "deepagent_dict[\"main\"].save_agent(path=model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:q-learning-lab] *",
   "language": "python",
   "name": "conda-env-q-learning-lab-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
